
@book{becker_carpentries_nodate,
	title = {The {Carpentries} {Curriculum} {Development} {Handbook}},
	url = {https://cdh.carpentries.org/},
	abstract = {This is a work in progress of the curriculum development handbook for The Carpentries.},
	urldate = {2023-09-01},
	author = {Becker, Erin and Michonneau, François},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/VHBDXGBU/cdh.carpentries.org.html:text/html},
}

@misc{noauthor_carpentries_nodate,
	title = {The {Carpentries} {Workbench}},
	url = {https://carpentries.github.io/workbench/},
	urldate = {2023-09-01},
	file = {The Carpentries Workbench:/Users/svenvanderburg/Zotero/storage/SSBZ6XPS/workbench.html:text/html},
}

@book{howard2020deep,
	title={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},
	author={Howard, J. and Gugger, S.},
	isbn={9781492045526},
	url={https://books.google.no/books?id=xd6LxgEACAAJ},
	year={2020},
	publisher={O'Reilly Media, Incorporated}
}

@misc{noauthor_udemy_nodate,
	title = {Udemy - {Basics} of {Deep} {Learning}},
	url = {https://www.udemy.com/course/basics-of-deep-learning/},
	abstract = {Fundamentals of Neural Network - Free Course},
	language = {en-us},
	urldate = {2023-09-01},
	journal = {Udemy},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/L57M3IZP/basics-of-deep-learning.html:text/html},
}

@misc{noauthor_udemy_nodate-1,
	title = {Udemy - {Tensorflow} 2.0 {\textbar} {Recurrent} {Neural} {Networks}, {LSTMs}, {GRUs}},
	url = {https://www.udemy.com/course/tensorflow-20-recurrent-neural-networks-lstms-grus/},
	abstract = {Sequence prediction course that covers topics such as: RNN, LSTM, GRU, NLP, Seq2Seq, Attention, Time series prediction - Free Course},
	language = {en-us},
	urldate = {2023-09-01},
	journal = {Udemy},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/GNQFE2IZ/tensorflow-20-recurrent-neural-networks-lstms-grus.html:text/html},
}

@misc{noauthor_udemy_nodate-2,
	title = {Udemy - {Data} {Science}: {Intro} {To} {Deep} {Learning} {With} {Python}},
	shorttitle = {Free {Deep} {Learning} {Tutorial} - {Data} {Science}},
	url = {https://www.udemy.com/course/complete-deep-learning-course-with-python/},
	abstract = {Learn to create Deep Learning Algorithms in Python - Free Course},
	language = {en-us},
	urldate = {2023-09-01},
	journal = {Udemy},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/EAUMLMBT/complete-deep-learning-course-with-python.html:text/html},
}


@misc{ng_deep_nodate,
	title = {Coursera - {Deep} {Learning}},
	url = {https://www.coursera.org/specializations/deep-learning},
	abstract = {Offered by DeepLearning.AI. Become a Machine Learning expert. Master the fundamentals of deep learning and break into AI. Recently updated ... Enroll for free.},
	language = {en},
	urldate = {2026-01-02},
	journal = {Coursera},
	author = {Ng, Andrew and Katanforoosh, Kian and Mourri, Younes},
}


@misc{noauthor_freecodecamporg_2022,
	title = {{freeCodeCamp}.org - {Learn} {PyTorch} for {Deep} {Learning}},
	url = {https://www.freecodecamp.org/news/learn-pytorch-for-deep-learning-in-day/},
	abstract = {My comprehensive PyTorch course is now live on the freeCodeCamp.org YouTube channel.   * You can view the full 26 hour course here [https://youtu.be/V\_xro1bcAuA].  * Read the course materials online for free at learnpytorch.io    [https://learnpytorch.io/].  * See all of the course materials on GitHub},
	language = {en},
	urldate = {2023-09-01},
	journal = {freeCodeCamp.org},
	month = oct,
	year = {2022},
}

@misc{noauthor_csc-_nodate,
	title = {{CSC}- {Practical} {Deep} {Learning}},
	url = {https://ssl.eventilla.com/event/8aPek},
	abstract = {Eventilla},
	language = {en},
	urldate = {2023-09-01},
	journal = {Eventilla},
}

@article{wilson_software_2006,
	title = {Software {Carpentry}: {Getting} {Scientists} to {Write} {Better} {Code} by {Making} {Them} {More} {Productive}},
	volume = {8},
	issn = {1558-366X},
	shorttitle = {Software {Carpentry}},
	doi = {10.1109/MCSE.2006.122},
	abstract = {For the past years, my colleagues and I have developed a one-semester course that teaches scientists and engineers the "common core" of modern software development. Our experience shows that an investment of 150 hours-25 of lectures and the rest of practical work-can improve productivity by roughly 20 percent. That's one day a week, one less semester in a master's degree, or one less year for a typical PhD. The course is called software carpentry, rather than software engineering, to emphasize the fact that it focuses on small-scale and immediately practical issues. All of the material is freely available under an open-source license at www.swc.scipy.org and can be used both for self-study and in the classroom. This article describes what the course contains, and why},
	number = {6},
	journal = {Computing in Science \& Engineering},
	author = {Wilson, G.},
	month = nov,
	year = {2006},
	note = {Conference Name: Computing in Science \& Engineering},
	keywords = {computation in undergraduate education, Computer science, continuing education, Debugging, Ethics, Java, Open source software, Physics, physics education, Portable computers, Programming profession, software engineering, Teamwork, World Wide Web},
	pages = {66--69},
}

@book{lang_small_2021,
	title = {Small {Teaching}: {Everyday} {Lessons} from the {Science} of {Learning}},
	isbn = {978-1-119-75554-8},
	shorttitle = {Small {Teaching}},
	abstract = {A freshly updated edition featuring research-based teaching techniques that faculty in any discipline can easily implement  Research into how we learn can help facilitate better student learning—if we know how to apply it. Small Teaching fills the gap in higher education literature between the primary research in cognitive theory and the classroom environment. In this book, James Lang presents a strategy for improving student learning with a series of small but powerful changes that make a big difference―many of which can be put into practice in a single class period. These are simple interventions that can be integrated into pre-existing techniques, along with clear descriptions of how to do so. Inside, you’ll find brief classroom or online learning activities, one-time interventions, and small modifications in course design or student communication. These small tweaks will bring your classroom into alignment with the latest evidence in cognitive research.  Each chapter introduces a basic concept in cognitive research that has implications for classroom teaching, explains the rationale for offering it within a specific time period in a typical class, and then provides concrete examples of how this intervention has been used or could be used by faculty in a variety of disciplines. The second edition features revised and updated content including a newly authored preface, new examples and techniques, updated research, and updated resources.   How can you make small tweaks to your teaching to bring the latest cognitive science into the classroom?  How can you help students become good at retrieving knowledge from memory?  How does making predictions now help us learn in the future?  How can you build community in the classroom?   Higher education faculty and administrators, as well as K-12 teachers and teacher trainers, will love the easy-to-implement, evidence-based techniques in Small Teaching.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Lang, James M.},
	month = aug,
	year = {2021},
	note = {Google-Books-ID: k8E4EAAAQBAJ},
	keywords = {Education / General, Education / Learning Styles, Education / Schools / Levels / Higher, Education / Teaching / General},
}

@misc{azalee_bostroem_software_2016,
	title = {Software {Carpentry}: {Programming} with {Python}.},
	url = {https://github.com/swcarpentry/python-novice-inflammation, 10.5281/zenodo.57492},
	abstract = {Programming with Python. Contribute to swcarpentry/python-novice-inflammation development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-09-01},
	journal = {GitHub},
	author = {{Azalee Bostroem} and {Trevor Bekolay} and {Valentina Staneva (eds)}},
	month = jun,
	year = {2016},
	note = {Version 2016.06},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/227MJWAZ/CITATION.html:text/html},
}

@misc{noauthor_scikit-learn_2023,
	title = {scikit-learn course},
	copyright = {CC-BY-4.0},
	url = {https://github.com/INRIA/scikit-learn-mooc},
	abstract = {Machine learning in Python with scikit-learn MOOC},
	urldate = {2023-09-01},
	publisher = {Inria},
	month = sep,
	year = {2023},
	note = {original-date: 2020-03-09T14:53:36Z},
	keywords = {machine-learning, mooc, python, scikit-learn},
}

@software{Pollard_Introduction_to_artificial_2022,
    author = {Pollard, Tom and Peru, Giacomo and Pontes Reis, Eduardo},
    month = may,
    title = {{Introduction to artificial neural networks in Python (Carpentries Incubator)}},
    url = {https://github.com/carpentries-incubator/machine-learning-neural-python},
    version = {0.1.0},
    year = {2022}
}

@misc{horst_allisonhorstpalmerpenguins_2020,
  title     = {allisonhorst/palmerpenguins: v0.1.0},
  url       = {https://doi.org/10.5281/zenodo.3960218},
  publisher = {Zenodo},
  author    = {Horst, Allison M. and Hill, Alison Presmanes and Gorman, Kristen B.},
  month     = jul,
  year      = {2020},
  doi       = {10.5281/zenodo.3960218}
}

@misc{huber_weather_2022,
	title = {Weather prediction dataset},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/4770936},
	doi = {10.5281/ZENODO.4770936},
	abstract = {Dataset created for machine learning and deep learning training and teaching purposes.{\textless}br{\textgreater} It can, for instance, be used for classification, regression, and forecasting tasks.{\textless}br{\textgreater} Complex enough to demonstrate realistic issues such as overfitting and unbalanced data, while still remaining intuitively accessible. {\textless}strong{\textgreater}Description and units of weather features:{\textless}/strong{\textgreater} Data includes the following features/variables for several European cities: Feature (type) Column name Description Physical Unit mean temperature \_temp\_mean mean daily temperature in 1 °C max temperature \_temp\_max max daily temperature in 1 °C min temperature \_temp\_min min daily temperature in 1 °C cloud\_cover \_cloud\_cover cloud cover oktas global\_radiation \_global\_radiation global radiation in 100 W/m2 humidity \_humidity humidity in 1 \% pressure \_pressure pressure in 1000 hPa precipitation \_precipitation daily precipitation in 10 mm sunshine \_sunshine sunshine hours in 0.1 hours wind\_speed \_wind\_gust wind gust in 1 m/s wind\_gust \_wind\_speed wind speed in 1 m/s {\textless}strong{\textgreater}File descriptions{\textless}/strong{\textgreater} {\textless}code{\textgreater}weather\_prediction\_dataset.csv{\textless}/code{\textgreater} - Main data file, tabular data, comma-separated CSV. Contains the data for different weather features (daily observations, see below for more details) for 18 European cities or places through the years 2000 to 2010. {\textless}code{\textgreater}weather\_prediction\_picnic\_labels.csv{\textless}/code{\textgreater} - Optional data to be used as potential labels for classification tasks. Contains booleans to characterize the daily weather conditions as suitable for a picnic (True) or not (False) for all 18 locations in the dataset. {\textless}code{\textgreater}weather\_prediction\_dataset\_map.png{\textless}/code{\textgreater}- Simple map showing all 18 locations in Europe. {\textless}code{\textgreater}metadata.txt{\textless}/code{\textgreater} - Further information on the dataset, the data processing, and conversion, as well as the description and units of all weather features. ORIGINAL DATA TAKEN FROM: EUROPEAN CLIMATE ASSESSMENT \&amp; DATASET (ECA\&amp;D), file created on 22-04-2021{\textless}br{\textgreater} THESE DATA CAN BE USED FREELY PROVIDED THAT THE FOLLOWING SOURCE IS ACKNOWLEDGED: Klein Tank, A.M.G. and Coauthors, 2002. Daily dataset of 20th-century surface{\textless}br{\textgreater} air temperature and precipitation series for the European Climate Assessment.{\textless}br{\textgreater} Int. J. of Climatol., 22, 1441-1453.{\textless}br{\textgreater} Data and metadata available at http://www.ecad.eu For more information see metadata.txt file.{\textless}br{\textgreater} The dataset has also been presented at the Teaching Machine Learning Workshop at ECML 2022: https://teaching-ml.github.io/2022/. The Python code used to create the weather prediction dataset from the ECA\&amp;D data can be found on GitHub: https://github.com/florian-huber/weather\_prediction\_dataset{\textless}br{\textgreater} (this repository also contains Jupyter notebooks with teaching examples) Versions: {\textless}strong{\textgreater}v5{\textless}/strong{\textgreater}: updated metadata.txt file. {\textless}strong{\textgreater}v4{\textless}/strong{\textgreater}: to be more future proof in times of climate change/crisis --\&gt; "BBQ weather" prediction is now "picnic weather" prediction. Data itself remains unchanged. {\textless}strong{\textgreater}v3{\textless}/strong{\textgreater}: added "light" version of the dataset with less features (only 11 locations and fewer variables, reduction from 163 to 89 features) --\&gt; This is meant to be used if training times for hands-on session is becoming an issues {\textless}strong{\textgreater}v2{\textless}/strong{\textgreater}: now also contains additional `BBQ\_weather` labels, the dataset itself has not changed between versions v1 and v2},
	language = {en},
	urldate = {2025-01-14},
	publisher = {Zenodo},
	author = {Huber, Florian and van Kuppevelt, Dafne and Steinbach, Peter and Sauze, Colin and Liu, Yang and Weel, Berend},
	month = sep,
	year = {2022},
	keywords = {machine learning, deep learning, training data, teaching material},
}

@article{gaviria_rojas_dollar_2022,
  title      = {The {Dollar} {Street} {Dataset}: {Images} {Representing} the {Geographic} and {Socioeconomic} {Diversity} of the {World}},
  volume     = {35},
  shorttitle = {The {Dollar} {Street} {Dataset}},
  url        = {https://papers.nips.cc/paper_files/paper/2022/hash/5474d9d43c0519aa176276ff2c1ca528-Abstract-Datasets_and_Benchmarks.html},
  language   = {en},
  urldate    = {2025-01-14},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Gaviria Rojas, William and Diamos, Sudnya and Kini, Keertan and Kanter, David and Janapa Reddi, Vijay and Coleman, Cody},
  month      = dec,
  year       = {2022},
  pages      = {12979--12990},
  file       = {Full Text PDF:/Users/carstenschnober/Zotero/storage/PJZDNZTV/Gaviria Rojas et al. - 2022 - The Dollar Street Dataset Images Representing the.pdf:application/pdf}
}


@article{huber_ms2deepscore_2021,
	title = {{MS2DeepScore}: a novel deep learning similarity measure to compare tandem mass spectra},
	volume = {13},
	issn = {1758-2946},
	shorttitle = {{MS2DeepScore}},
	url = {https://doi.org/10.1186/s13321-021-00558-4},
	doi = {10.1186/s13321-021-00558-4},
	abstract = {Mass spectrometry data is one of the key sources of information in many workflows in medicine and across the life sciences. Mass fragmentation spectra are generally considered to be characteristic signatures of the chemical compound they originate from, yet the chemical structure itself usually cannot be easily deduced from the spectrum. Often, spectral similarity measures are used as a proxy for structural similarity but this approach is strongly limited by a generally poor correlation between both metrics. Here, we propose MS2DeepScore: a novel Siamese neural network to predict the structural similarity between two chemical structures solely based on their MS/MS fragmentation spectra. Using a cleaned dataset of {\textgreater} 100,000 mass spectra of about 15,000 unique known compounds, we trained MS2DeepScore to predict structural similarity scores for spectrum pairs with high accuracy. In addition, sampling different model varieties through Monte-Carlo Dropout is used to further improve the predictions and assess the model’s prediction uncertainty. On 3600 spectra of 500 unseen compounds, MS2DeepScore is able to identify highly-reliable structural matches and to predict Tanimoto scores for pairs of molecules based on their fragment spectra with a root mean squared error of about 0.15. Furthermore, the prediction uncertainty estimate can be used to select a subset of predictions with a root mean squared error of about 0.1. Furthermore, we demonstrate that MS2DeepScore outperforms classical spectral similarity measures in retrieving chemically related compound pairs from large mass spectral datasets, thereby illustrating its potential for spectral library matching. Finally, MS2DeepScore can also be used to create chemically meaningful mass spectral embeddings that could be used to cluster large numbers of spectra. Added to the recently introduced unsupervised Spec2Vec metric, we believe that machine learning-supported mass spectral similarity measures have great potential for a range of metabolomics data processing pipelines.},
	number = {1},
	urldate = {2025-02-11},
	journal = {Journal of Cheminformatics},
	author = {Huber, Florian and van der Burg, Sven and van der Hooft, Justin J. J. and Ridder, Lars},
	month = oct,
	year = {2021},
	keywords = {Deep learning, Mass spectrometry, Metabolomics, Spectral similarity measure, Supervised machine learning},
	pages = {84},
	file = {Full Text PDF:/Users/svenvanderburg/Zotero/storage/Y3KAXM5F/Huber et al. - 2021 - MS2DeepScore a novel deep learning similarity mea.pdf:application/pdf;Snapshot:/Users/svenvanderburg/Zotero/storage/BIH5UWCE/s13321-021-00558-4.html:text/html},
}

@misc{van_der_burg_dollar_2024,
	title = {Dollar street 10 - 64x64x3},
	url = {https://zenodo.org/records/10970014},
	doi = {10.5281/zenodo.10970014},
	abstract = {The MLCommons Dollar Street Dataset is a collection of images of everyday household items from homes around the world that visually captures socioeconomic diversity of traditionally underrepresented populations. It consists of public domain data, licensed for academic, commercial and non-commercial usage, under CC-BY and CC-BY-SA 4.0. The dataset was developed because similar datasets lack socioeconomic metadata and are not representative of global diversity.

This is a subset of the original dataset that can be used for multiclass classification with 10 categories. It is designed to be used in teaching, similar to the widely used, but unlicensed CIFAR-10 dataset.

These are the preprocessing steps that were performed:



Only take examples with one imagenet\_synonym label

Use only examples with the 10 most frequently occuring labels

Downscale images to 64 x 64 pixels

Split data in train and test

Store as numpy array


This is the label mapping:




Category
label


day bed
0


dishrag
1


plate
2


running shoe
3


soap dispenser
4


street sign
5


table lamp
6


tile roof
7


toilet seat
8


washing machine
9




Checkout this notebook to see how the subset was created.

The original dataset was downloaded from https://www.kaggle.com/datasets/mlcommons/the-dollar-street-dataset. See https://mlcommons.org/datasets/dollar-street/ for more information.},
	urldate = {2025-02-11},
	publisher = {Zenodo},
	author = {van der burg, Sven},
	month = apr,
	year = {2024},
	keywords = {CC-BY, CIFAR-10, Deep learning, Image classification, Machine learning},
	file = {Snapshot:/Users/svenvanderburg/Zotero/storage/QPJDIYXH/10970014.html:text/html},
}


@article{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	language = {en},
	author = {Krizhevsky, Alex},
	year = {2009},
}


